---
title: "2. Współliniowość, CV, over/under-fitting"
author: "Michał Maj"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: united
    number_sections: true
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mctest)
```

Skoro wiemy już czym jest i jak działa model regresji liniowej warto się zastanowić na co powinnśmy zwrócić uwagę podczas modelowania (zasady te będą równie ważne podaczas używania innych modeli niż liniowy).

# Współliniowość zmiennych

Budując model powinniśmy zwrócić uwagę na to czy nasze predyktory nie są współliniowe (jeden z predyktorów jest kombinacją liniową innych). Wprowadzanie współliniowych predyktorów powoduje przekazywanie identycznej informacji z wielu miejsc:

```{r, message=FALSE}
set.seed(33)
colinearity <- read_csv("colinearity.csv")
lm(formula = y ~ x1 + x2, data = colinearity) %>% summary()
lm(formula = y ~ x1 + x2 + z, data = colinearity) %>% summary()
```

Objawy współliniowosci:

 - Duże zmiany współczynników regresji pod dodaniu/odjęciu zmiennej.
 - Wysoka wartość predykcyjna modelu (np wysokie R^2) lecz niesistotne zmienne.

Najprostszym sposobem badania współliniowości predyktorów jest policzenie korelacji:

```{r}
cor(colinearity[ , 1:3])
```

Jeśli widzimy 2 mocno skorelowane ze sobą predyktory to powinnismy pozbyć się jednego z nich. Niestety ten sposób nie działa gdy jeden predyktor jest kombinacją pozostałych.

```{r}
cor(colinearity$z, -2.37 * colinearity$x1  + 3.33 * colinearity$x2 + 1)
```

Innym sposobem jest policzenie współczynnika VIF (Variance inflation factor). w skrócie dla danedo predyktora tworzymy model liniowy zakładając zależność od pzostałych predyktórw. Nastepnie liczymy VIF jako `1 / (1 - R^2)`:

```{r}
lm(formula = z ~ x1 + x2, data = colinearity) %>% summary()
vif_z = 1 / (1 - 0.9939) # VIF > 5 - duża współliniowosć, wyrzucamy zmienną
vif_z
```

Ostatnim sposobem jest Test Farrar'a–Glauber'a:

```{r}
imcdiag(lm(formula = y ~ x1 + x2 + z, data = colinearity))
imcdiag(lm(formula = y ~ x1 + x2, data = colinearity))
imcdiag(lm(formula = y ~ x1 + z, data = colinearity))
imcdiag(lm(formula = y ~ x2 + z, data = colinearity))
```

# Under/Overfitting

Zadaniem modelu są 2 rzeczy:

1. Optymalizacja - Regresja ma być dobrze dopasowana do danych, które mamy. To co dotąd opisywaliśmy.
2. Generalizacja - Regresja ma takze dawać poprawne predykcje dla nowych danych.

Zacznijmy od wygenerowania nowych danych:

```{r}
set.seed(44)
dane <- tibble(x = seq(-2, 6, length.out = 20) + runif(20, -3, 3),
               y = 1.1 * x^2 - x + 3 + rnorm(20, 0, 2))
dane_test <- tibble(x = seq(-2, 6, length.out = 10) + runif(10, -3, 3),
               y = 1.1 * x^2 - x + 3  + rnorm(10, 0, 2))
p <- ggplot(dane, aes(x, y)) + geom_point() + geom_point(data = dane_test, color = "red") + theme_bw()
p
```

Przetestujemy 3 modele liniowe:

```{r}
model_lin <- lm(data = dane, y ~ x)
model_squared <- lm(data = dane, y ~ x + I(x^2))
model_poly7 <- lm(data = dane, y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7))

preds <- tibble(x = seq(-3, 8, by = 0.1)) %>%
  mutate(pred_lin = predict(model_lin, .),
         pred_squared = predict(model_squared, .),
         pred_poly7 = predict(model_poly7, .))
p + geom_line(data = preds, aes(x, pred_lin), color = "blue")
p + geom_line(data = preds, aes(x, pred_squared), color = "blue")
p + geom_line(data = preds, aes(x, pred_poly7), color = "blue")
p + geom_line(data = preds[1:90, ], aes(x, pred_poly7), color = "blue")
```

I policzymy predykcje dla zbioru treningowego (dane, które mamy do treningu) i testowego (danych nowych):

```{r}
"Błedy na zbiorze treningowym:"
sqrt(sum((predict(model_lin, dane) - dane$y)^2))
sqrt(sum((predict(model_squared, dane) - dane$y)^2))
sqrt(sum((predict(model_poly7, dane) - dane$y)^2)) # Najmniejszy
"Błedy na zbiorze testowym:"
sqrt(sum((predict(model_lin, dane_test) - dane_test$y)^2))
sqrt(sum((predict(model_squared, dane_test) - dane_test$y)^2)) # Najmniejszy
sqrt(sum((predict(model_poly7, dane_test) - dane_test$y)^2))
```

Model liniowy nie był dobrze dopasowany do danych treningowych, duży błąd na zbiorze treningowym i testowym moze swiadczyć o **underfittingu** modelu. Oznacza to, że wybrany przez nasz model jest zbyt prosty, zbyt mało elastyczny do modelowanego zjawiska. Rozwiazaniem jest uelastycznienie naszego modelu (wybór innej metody np. sieci neuronowe, drzewa itp. lub dodanie nieliniowosci). 

Model wielomianowy ma niski błąd na zbiorze treningowym, jest bardzo dobrze dopasowany do danych, jednakże wypada bardzo kiepsko na nowych danych (nie potrafi generalizować). jest to swiadectwo **overfittingu**. Rozwiązaniem tego problemu jest **penalizacja modelu** (np metoda LASSO lub Ridge), lub wybranie mniej elastycznego modelu.

Model kwadratowy ma niski błąd na obu zbiorach. Widac, że udało się otrzymac kompromis między zadaniami optymalizacji i generalizacji.

Przy tworzeniu jakiegokolwiek modelu nie powinno się go budować na całosci danych! Dane należy podzielić na conajmniej 2 zbiory - **treningowy** i **testowy**. Modele powinny być budowane na zbiorze treningowycm i porównywane na testowym. 

# Crossvalidation

Bardzo potężną metodą (a zarazem bardzo prostą), która może pomóc nam w ustaleniu prawdziwej wartości błędu jest **Crossvalidation**.

Działa ona w nastepujący sposób. Nasz zbiór treningowy dzielimy na N równych częsci - tak zwanych **fold'ów**. Dla przykładu na 5:

```{r}
dane_cv <- dane %>%
  mutate(cv_fold = (row_number() - 1) %% 5)
lin_error <- c()
sq_error <- c()
poly7_error <- c()
```

Nastepnie zbudujemy N modeli w taki sposób, ze 1 fold nie będzie brał udziału w treningu (taki zbiór testowy), a model zbudujemy na N-1 foldach. Nastepnie policzymy błąd na foldzie, który nie brał udziału w treningu. Otrzymamy w ten sposób N błędów. Ostateczny estymat błędu będzie średnią z tych błędów:

```{r}
for (fold in 0:4) {
  train <- dane_cv %>% filter(cv_fold != fold) %>% select(-cv_fold)
  test <- dane_cv %>% filter(cv_fold == fold) %>% select(-cv_fold)
  model_lin <- lm(data = train, y ~ x)
  model_squared <- lm(data = train, y ~ x + I(x^2))
  model_poly7 <- lm(data = train, y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7))
  lin_error <- c(lin_error, sqrt(sum((predict(model_lin, test) - test$y)^2)))
  sq_error <- c(sq_error, sqrt(sum((predict(model_squared, test) - test$y)^2)))
  poly7_error <- c(poly7_error, sqrt(sum((predict(model_poly7, test) - test$y)^2)))
}

mean(lin_error)
mean(sq_error) # Najmniejszy
mean(poly7_error)
```

Procedura ta to tak zwana **N-fold Crossvalidation**. 

Można by się zastanowić co stanie się jeśli nasz fold będzie tylko pojedynczą obserwacja? Mamy wtedy do czyniena z tak zwana **Leave-one-out Crossvalidation**. Jej zaletą jest to że dostajemy lepszy estymat błędu, wadą jest złożoność obliczeń, musimy budować tyle modeli ile jest obserwacji. 

```{r}
dane_cv <- dane %>%
  mutate(cv_fold = row_number())
lin_error <- c()
sq_error <- c()
poly7_error <- c()

for (fold in 1:20) {
  train <- dane_cv %>% filter(cv_fold != fold) %>% select(-cv_fold)
  test <- dane_cv %>% filter(cv_fold == fold) %>% select(-cv_fold)
  model_lin <- lm(data = train, y ~ x)
  model_squared <- lm(data = train, y ~ x + I(x^2))
  model_poly7 <- lm(data = train, y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7))
  lin_error <- c(lin_error, sqrt(sum((predict(model_lin, test) - test$y)^2)))
  sq_error <- c(sq_error, sqrt(sum((predict(model_squared, test) - test$y)^2)))
  poly7_error <- c(poly7_error, sqrt(sum((predict(model_poly7, test) - test$y)^2)))
}

mean(lin_error)
mean(sq_error) # Najmniejszy
mean(poly7_error)
```


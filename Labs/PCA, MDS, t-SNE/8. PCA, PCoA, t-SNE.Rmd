---
title: "8. PCA, PCoA, t-SNE"
author: "Michał Maj"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: united
    number_sections: true
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(plotly)
library(ggbiplot)
library(Rtsne)
library(gridExtra)
```

# Redukcja wymiarów

## Wstęp

Zwizualizujmy sobie nastepujacy zestaw danych:

```{r, message=FALSE, warning=FALSE}
test_data <- tibble(
  x1 = sample(seq(0, 10, by = 0.1), 50),
  x2 = sample(seq(0, 30, by = 0.1), 50),
  col = "Real"
) %>%
  mutate(x3 = 2 * x1 + 6 * x2 - 3 + rnorm(50, 2, 16))

plot_ly(data = test_data, x = ~x1, y = ~x2, z = ~x3,
        size = I(4), color = ~col, colors = c("red"))

test_data_manifold <- expand.grid(seq(0, 10, by = 0.1), seq(0, 30, by = 0.1)) %>%
  dplyr::rename(x1 = Var1, x2 = Var2) %>%
  mutate(x3 = 2 * x1 + 6 * x2 - 3) %>%
  mutate(col = "Manifold") %>%
  bind_rows(., test_data)

plot_ly(data = test_data_manifold, x = ~x1, y = ~x2, z = ~x3,
        size = I(4), color = ~col, colors = c("green", "red"))
```

oraz:

```{r, message=FALSE, warning=FALSE}
test_data2 <- tibble(
  x1 = sample(seq(0, 10, by = 0.1), 50),
  x2 = sample(seq(0, 30, by = 0.1), 50),
  col = "Real"
) %>%
  mutate(x3 = 100*sin(x1)*cos(x2) + rnorm(50, 2, 8))

plot_ly(data = test_data2, x = ~x1, y = ~x2, z = ~x3,
        size = I(4), color = ~col, colors = c("red"))

test_data_manifold2 <- expand.grid(seq(0, 10, by = 0.1), seq(0, 30, by = 0.1)) %>%
  dplyr::rename(x1 = Var1, x2 = Var2) %>%
  mutate(x3 = 100*sin(x1)*cos(x2)) %>%
  mutate(col = "Manifold") %>%
  bind_rows(., test_data2)

plot_ly(data = test_data_manifold2, x = ~x1, y = ~x2, z = ~x3,
        size = I(4), color = ~col, colors = c("green", "red"))
```

W obu przypadkach dane, które posiadamy są trójwymiarone, jednakże łatwo pokazać, że istnieje pewien dwuwymiarowy **manifold**, który przybliża nam prawdziwy rozkład danych - po zrzutowaniu na manifold nie tracimy zbyt wiele informacji (wariancji) oryginalnych danych.

W przypadku 3 wymiarów ta informacja nie jest zbytnio interesująca, jednakże gdy mamy do czynienia z dużą liczbą zmiennych (predyktorów) zrzutowanie ich do nisko-wymiarowej podprzestrzeni może być bardzo korzystne. Jest to tak zwana **redukcja wymiarów**.

Z reguły metody redukcji wymiarów dzielą się na **liniowe** jak na przykład **analiza głównych składowych (PCA)**, **analiza składowych niezależnych (ICA)**, **liniowe skalowanie wielowymiarowe** oraz **nieliniowe** jak na przykład **nie liniowe skalowanie wielowymiarowe**, **UMAP**, **t-SNE**.

Redukcja wymiarów przydaje się do:

- wizualizacji danych
- redukcji overfittingu (pozbycie sie klątwy wymiarór)
- selekcji/tworzenia zmiennych
- zakodowania zbioru danych

## Analiza głównych składowych (PCA)

Przyjmijmy, że mamy $p$ zmiennych, $X_j, \ (j = 1, ..., p)$, a obserwacje przeprowadzone na $n$ jednorodnych obiektach $(i = 1, ..., n)$ zebrane są w formie macierzy danych $X$, o której zakładamy, że jest pełnego rzędu (aby nie zmieniać struktury zmiennych jakąkolwiek liniową ich kombinacją). Zdefiniujmy dla tych zmiennych macierze zależności: macierz korelacji $R$ i macierz kowariancji $S$. Zakłada się przy tym, że macierze korelacji i kowariancji mają pewną liczbę różnych największych **wartości własnych**.

Idea **analizy głównych składowych** (PCA) polega na ortogonalnej transformacji układu badanych zmiennych $X_j$ w zbiór nowych nieobserwowanych zmiennych $Y_l$ które są liniowymi kombinacjami tych obserwowanych zmiennych, co możemy zapisać w postaci układu równań:

$$
\begin{array}{c}
Y_{1}=w_{11} X_{1}+w_{21} X_{2}+\ldots+w_{p 1} X_{p} \\
Y_{2}=w_{12} X_{1}+w_{22} X_{2}+\ldots+w_{p 2} X_{p} \\
\cdots \\
Y_{m}=w_{1 m} X_{1}+w_{2 m} X_{2}+\ldots+w_{p m} X_{p}\\
Y_{k} \perp Y_{l}, \ k\neq l
\end{array}
$$

W postaci uogólnionej układ ten zapiszemy:

$$
Y_{l}=w_{1}, X_{1l}+w_{2} X_{2l}+\cdots+w_{p l} X_{p}=\sum_{j=1}^{p} w_{j l} X_{j}
$$

Nowe, przetworzone zmienne $Y_l$ noszą nazwę **głównych składowych** (ang. principal components) zmiennych $X_j$ lub też zmiennych składowych, zaś współczynniki $w_{jl}$ nazywają się **ładunkami składowymi** (ang. component loadings). Są one **nieskorelowane między sobą** (ortogonalne) i unormowane (suma kwadratów współczynników danej kombinacji $w_{jl}$ jest równa jeden), a suma wariancji składowych $Y_l$ jest równa ogólnej wariancji zmiennych $X_j$.

Podstawowy cel analizy głównych składowych, polega na identyfikacji struktury zależności, poprzez utworzenie zupłnie nowego zbioru istotnych zmiennych, który częściowo bądź całkowicie mógłby zastąpic pierwotny zbiór zmiennych. Cel ten określa sie jako redukcję wymiarowości (ang. reduction of dimensionality lub reduction of the basic dimensions) złozonego zjawiska. Dobrze jest bowiem, gdy złozoną strukture zaleznosci uda sie opisac niewielką liczbą głównych składowych, z możliwością ich
dalszego wykorzystania w innych technikach analizy wielowymiarowej.

Istnieje kilka różnych procedur uzyskiwania głównych składowych. Najpowszechniej stosowana jest **metoda Hotellinga** (1933), wykorzystująca **metodę mnożników Lagrange'a** maksymalizacji funkcji wielu zmiennych. Prześledzimy ją, przyjmując najpierw, że punktem wyjścia analizy jest macierz kowariancji $S$.

Rozważmy pierwszą główną składow¡ modelu:

$$
Y_{1}=w_{11} X_{1}+w_{21} X_{2}+\ldots+w_{p 1} X_{p} = \mathbf{w}_{1}^{\prime}x
$$
gdzie $\mathbf{w}_{1}^{\prime} = [w_{11}, ..., w_{p1}]$  której wariancja wynosi:

$$
S^{2}\left(Y_{1}\right)=\sum_{k=1}^{p} \sum_{j=1}^{p} w_{j 1} w_{k 1} s_{j k}=\mathbf{w}_{1}^{\prime} \mathbf{S} \mathbf{w}_{1}
$$

Jest to więc funkcja $p$ wspólczynników $w_{11}, ..., w_{p1}$, które musza być wybrane tak aby maksymalizować wariancję przy warunku $\mathbf{w}_{1}^{\prime}\mathbf{w}_{1}=1$. Warunek ten wprowadza ograniczenie na wartości $w_{j1}$ tak aby długość wektora $\mathbf{w}$ była równa $1$. Jest to tak zwany warunek normalizujący.

Do znalezienia wartości współczynników $\mathbf{w}_{1}^{\prime}$ stosuje się metodę Lagrange'a (metoda optmalizacyjna do znajdowania ekstremów funkcji różniczkowalnej). Oznaczmy przez $\lambda_1$ mnoznik oraz zdefiniujmy funkcje pomocniczą uwzględniającą ograniczenie normalizacyjne:

$$
\varphi=1-\mathbf{w}_{1}^{\prime} \mathbf{w}_{1} \equiv 0
$$

Utwórzmy funkcję Lagrange'a:

$$
L\left(\mathbf{w}_{1}\right)=S^{2}\left(Y_{1}\right)+\lambda_{1}\left(1-\mathbf{w}_{1}^{\prime} \mathbf{w}_{1}\right)=\mathbf{w}_{1}^{\prime} \mathbf{S} \mathbf{w}_{1}+\lambda_{1}\left(1-\mathbf{w}_{1}^{\prime} \mathbf{w}_{1}\right)
$$

a obliczoną pochodną względem wektora $w_1$, porównajmy do zera:

$$
\frac{\partial L}{\partial \mathbf{w}_{1}}=2 \mathbf{S} \mathbf{w}_{1}-2 \lambda_{1} \mathbf{w}_{1}=2\left(S-\lambda_{1} \mathrm{I}\right) \mathbf{w}_{1}=0
$$

A zatem poszukiwane współczynniki $w_1$, muszą spełniac $p$ jednorodnych równań liniowych:

$$
(S-\lambda_{1} \mathrm{I})\mathbf{w}_{1}=0
$$
Ponieważ rozwiązaniem nie może być wektor zerowy to $\lambda_1$ musi być liczbą spełniającą równanie wyznacznikowe:

$$
|S-\lambda_{1} \mathrm{I}|\mathbf{w}_{1}=0
$$
Z powyższego widać, że $\lambda_1$ musi być **wartością własną** macierzy $S$, zaś wektor $w_1$ jest zwiazanym z nią $wektorem własnym$:

$$
S\mathbf{w}_{1}=\lambda_{1}\mathbf{w}_{1}
$$
Mnożąc powyższe równanie lewostronnie przez $\mathbf{w}_{1}^{\prime}$ i wykorzystując ograniczenie normalizujące dostajemy:

$$
\mathbf{w}_{1}^{\prime} \mathbf{S} \mathbf{w}_{1}=\lambda_{1} \mathbf{w}_{1}^{\prime} \mathbf{w}_{1}=\lambda_{1}=S^{2}\left(Y_{1}\right)
$$

Ponieważ wektor współczynników ma być wybrany tak aby maksymalizować wariancje to $\lambda_1$ musi być **największa wartością własną** macierzy $S$. Pierwsza główna składowa jest zatem wyznaczona przez parę: wartość włansa - wektor własny $(\lambda_1, \mathbf{w}_{1})$

Współczynniki drugiej głównej składowej wyznaczamy jak poprzednio, metodą Lagrange'a, przy czym funkcja Lagrange'a jest tym razem dana wzorem uwzględniającym dwa ograniczenia:

$$
\begin{aligned}
L\left(\mathbf{w}_{2}\right) &=S^{2}\left(Y_{2}\right)+\lambda_{2}\left(1-\mathbf{w}_{2}^{\prime} \mathbf{w}_{2}\right)+\pi \mathbf{w}_{1}^{\prime} \mathbf{w}_{2}=\\
&=\mathbf{w}_{2}^{\prime} \mathbf{S} \mathbf{w}_{2}+\lambda_{2}\left(1-\mathbf{w}_{2}^{\prime} \mathbf{w}_{2}\right)+\pi \mathbf{w}_{1}^{\prime} \mathbf{w}_{2}
\end{aligned}
$$
gdzie $\pi$ jest drugim mnoznikiem, a $\mathbf{w}_{1}^{\prime} \mathbf{w}_{2} \equiv 0$ jest drugą funkcją pomocniczą (warunek ortogonalności). Rozwiązując równanie w analogiczny sposób dojdziemy do rozwiązaniem będącego **drugą co do wielkości wartością własną**. Analogicznie kolejnymi rozwiązaniami byłyby kolejne wartości własne, rozwiązanie wiec sprowadza się do znalezienia wartości własnych macierzy $S$, bedącymi pierwiastkami równaia:

$$
|S-\lambda \mathrm{I}|=0
$$
i uporządkowanie ich malejąco (zazwyczaj potrzebna jest także normalizacja).

Wagę l−tej głównej składowej mierzymy ilorazem:

$$
\mathrm{I}\left(Y_{l}\right)=\frac{\lambda_{l}}{\operatorname{tr} \mathrm{S}}(100 \%)
$$

który informuje nas jaki odsetek wariancji tłumaczony jest przez l-tą składową.

Wspóªczynniki głównych składowych $w_{jl}$ zasługują na uważną analizę. Ponieważ po normalizacji suma ich kwadratów jest równa jeden

$$
\sum_{j=1}^{p} w_{j l}^{2}=1, \quad l=1, \ldots, p
$$

są one wagami, z jakimi poszczególne zmienne $X_j$ komponują się wskładowe i mierzą tym samym znaczenie zmiennych w składowych. Znak współczynnika informuje o sposobie, zas moduł o wielko±ci wpływu j−tej zmiennej na l−tą gªówn¡ składową.

Z geometrycznego punktu widzenia ideą analizy głównych składowych jest opisanie zmienności układu n punktów w p−wymiarowej przestrzeni cech poprzez wprowadzenie nowego układu liniowych, ortogonalnych współrzędnych. Wariancje danych punktów względem wprowadzonych współrzędnych są uporządkowane malejąco. Rzuty punktów na pierwszą składową mają największą wariancję ze wszystkich możliwych liniowych współrzędnych.

Sprawdźmy jak PCA zadziała dla przykładowych danych motoryzacyjnych:

```{r}
mtcars.pca <- prcomp(mtcars[,c(1:7, 10, 11)], center = TRUE, scale. = TRUE)
```

Zacznijmy od zwizualizowania dwóch pierwszych składowych oraz tłumaczonej wariancji:

```{r}
summary(mtcars.pca)
mtcars.country <- c(rep("Japan", 3), rep("US",4), rep("Europe", 7),rep("US",3), "Europe", rep("Japan", 3), rep("US",4), rep("Europe", 3), "US", rep("Europe", 3))
ggbiplot(mtcars.pca, ellipse = TRUE,  labels = rownames(mtcars), groups = mtcars.country)
```

Zalety PCA:
 - zachowanie globalnej struktury danych
 - możliwość policzenia wartości składowych dla nowej obserwacji
 - łatwość interpretacji składowych (funckja liniowa orginałów)

Wady PCA:
 - nie zachowuje lokalnej struktury danych (clustry)
 - działa tylko dla liniowych danych
 - trudność w interpretacji (każda składowa zalezna od każdej orginalnej zmiennej - alternatywą jest **regularized PCA**)
 
Główne składowe mogą zostać uzyte jako nowe zmienne w dalszej analizie, np. w regresji lub klasyfikacji. jest to tak zwana **principal components regression/classification**
 
Na zakończenie pozostaje nam jeszcze zobaczyć jak PCA policzyć ręcznie:

```{r}
# Skalowanie zmiennych
iris_data <- iris %>% select(-Species) %>% scale(center = TRUE, scale = TRUE) %>% as.data.frame()
# Wyliczenie macierzy kowariancji/korelacji
cor_iris_data <- cor(iris_data)
# Wyliczenie wartości własnych
eign_iris_data <- eigen(cor_iris_data)
# Wyliczenie składowych
pc1 <- iris_data$Sepal.Length * eign_iris_data$vectors[1,1] + iris_data$Sepal.Width * eign_iris_data$vectors[2,1] +
  iris_data$Petal.Length * eign_iris_data$vectors[3,1] + iris_data$Petal.Width * eign_iris_data$vectors[4,1]

pc2 <- iris_data$Sepal.Length * eign_iris_data$vectors[1,2] + iris_data$Sepal.Width * eign_iris_data$vectors[2,2] +
  iris_data$Petal.Length * eign_iris_data$vectors[3,2] + iris_data$Petal.Width * eign_iris_data$vectors[4,2]

pc3 <- iris_data$Sepal.Length * eign_iris_data$vectors[1,3] + iris_data$Sepal.Width * eign_iris_data$vectors[2,3] +
  iris_data$Petal.Length * eign_iris_data$vectors[3,3] + iris_data$Petal.Width * eign_iris_data$vectors[4,3]

pc4 <- iris_data$Sepal.Length * eign_iris_data$vectors[1,4] + iris_data$Sepal.Width * eign_iris_data$vectors[2,4] +
  iris_data$Petal.Length * eign_iris_data$vectors[3,4] + iris_data$Petal.Width * eign_iris_data$vectors[4,4]

iris_pc <- data.frame(pc1, pc2, pc3, pc4)
plot(iris_pc[, 1:2], col = iris[, 5])
```

## Analiza głównych koordynatów (PCoA) - klasyczne skalowanie wielowymiarowe (cMDS)

**Analiza głównych koordynatów** (PCoA) nazywana też klasycznym skalowaniem wielowymiarowym (cMDS) jest uogólnieniem PCA. Algorytm nalizy jest praktycznie identyczny, jednakze zamiast uzywac macierzy korelacji/kowariacnji używamy dowolnej **macierzy odległości** (zauważ, że użycie odległości euklidesowej jest równowazne z uzyciem macierzy korelacji co daje PCA).

Sprawdźmy różnicę między PCA i PCoA dla pewnych danych genetycznych:

```{r}
gene_data <- read_csv("gene_data.csv")
rownames(gene_data) <- c("wt1", "wt2", "wt3", "wt4", "wt5", "ko1", "ko2", "ko3", "ko4", 
"ko5")

# PCA
pca <- prcomp(gene_data, scale = TRUE, center = TRUE)
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per

pca.data <- data.frame(
  Sample = rownames(pca$x),
  PC1 = pca$x[,1],
  PC2 = pca$x[,2])
 
ggplot(data=pca.data, aes(x=PC1, y=PC2, label=Sample)) +
  geom_text() +
  xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
  ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
  theme_bw() +
  ggtitle("PCA Graph")

# PCoA - dystans euklidesowy
distance.matrix <- dist(scale(gene_data, center = TRUE, scale = TRUE), method = "euclidean")
mds <- cmdscale(distance.matrix, eig = TRUE, x.ret = TRUE)

mds.var.per <- round(mds$eig/sum(mds$eig)*100, 1)
mds.var.per
 
mds.values <- mds$points
mds.data <- data.frame(
  Sample = rownames(mds.values),
  PCo1 = mds.values[, 1],
  PCo2 = mds.values[, 2])
 
ggplot(data = mds.data, aes(x = PCo1, y = PCo2, label = Sample)) +
  geom_text() +
  theme_bw() +
  xlab(paste("MDS1 - ", mds.var.per[1], "%", sep="")) +
  ylab(paste("MDS2 - ", mds.var.per[2], "%", sep="")) +
  ggtitle("MDS plot using Euclidean distance")
```

Jak widać oba wykresy dają tę samą odpowiedź. sprawdźmy teraz co stanie się gdy użyjemy innej odległości. W genetyce czesto stosowana jest **Log Fold Change**:

$$
log_2FC(sample_1, sample_2) = |log_2(sample_1) - log_2(sample_2)|
$$
```{r}
log2.data.matrix <- log2(gene_data)

log2.distance.matrix <- matrix(0,
  nrow=ncol(log2.data.matrix),
  ncol=ncol(log2.data.matrix),
  dimnames=list(colnames(log2.data.matrix),
    colnames(log2.data.matrix)))
 
log2.distance.matrix
 
for(i in 1:ncol(log2.distance.matrix)) {
  for(j in 1:i) {
    log2.distance.matrix[i, j] <-
      mean(abs(log2.data.matrix[,i] - log2.data.matrix[,j]))
  }
}
log2.distance.matrix

mds2 <- cmdscale(as.dist(log2.distance.matrix), eig = TRUE, x.ret = TRUE)
 
mds.var.per <- round(mds2$eig/sum(mds2$eig)*100, 1)
mds.var.per
 
mds.values <- mds2$points
mds.data <- data.frame(
  Sample = rownames(mds.values),
  PCo1 = mds.values[,1],
  PCo2 = mds.values[,2])
 
ggplot(data=mds.data, aes(x = PCo1, y = PCo2, label=Sample)) +
  geom_text() +
  theme_bw() +
  xlab(paste("MDS1 - ", mds.var.per[1], "%", sep="")) +
  ylab(paste("MDS2 - ", mds.var.per[2], "%", sep="")) +
  ggtitle("MDS plot using avg(logFC) as the distance")
```

## t-SNE

Metody takie jak PCA czy MDS działają dla nieskomplikowanych danych tabularycznych, lecz co jeśli chcielibyśmy redukcję wymiarów przeprowadzić dla tekstów, zdjęc lub innych złozonych danych? Na szczeście istnieje wiele nieliniowych metod, które mogą okazać się pomocne w takim przypadku.

**t-distributed stochastic neighbor embedding (t-SNE)** jest jedną z popularniejszych metod tego typu. Załóżmy, że mamy mamy $N$ obserwacji w przestrzeni p-wymiarowej. t-SNE stara się nauczyc niskowymiarowej reprezentacji (mapy) poprzez minimalizację **dywergencji Kullbacka–Leiblera**, będącej pseudo-odległością między rozkładami.

W kroku pierwszym liczone sa prawdopodobieństwa $p_{ij}$ proporcjonalne do podobieństwa między obserwacjami $x_i$ i $x_j$:

$$
p_{j \mid i}=\frac{\exp \left(-\left\|\mathbf{x}_{i}-\mathbf{x}_{j}\right\|^{2} / 2 \sigma_{i}^{2}\right)}{\sum_{k \neq i} \exp \left(-\left\|\mathbf{x}_{i}-\mathbf{x}_{k}\right\|^{2} / 2 \sigma_{i}^{2}\right)}\\
p_{i \mid i} = 0
$$
a ostatecznie:

$$
p_{ij} = \frac{p_{j \mid i} + p_{i \mid j}}{2n}
$$

Powyższe wzory można roumiec w następujący sposób: podobienstwo punktu $x_j$ do $x_i$ jest wyrażone poprzez prawdopodobieństwo warunkowe $p_{j \mid i}$ tego, ze $x_i$ wybrał by $x_j$ jako swojego sąsiada, jesli sąsiedzi wybierani byliby proporcjonalnie względem rozkładu gausssowskiego z centrum w $x_i$.

Nastepnie t-SNE stara się odwzorować powyższe podobieństwa w przestrzenie niskowymiarowej. Podobienstwa w tej przestrzeni liczone są za pomocą wzoru:

$$
q_{i j}=\frac{\left(1+\left\|\mathbf{y}_{i}-\mathbf{y}_{j}\right\|^{2}\right)^{-1}}{\sum_{k} \sum_{l \neq k}\left(1+\left\|\mathbf{y}_{k}-\mathbf{y}_{l}\right\|^{2}\right)^{-1}}\\
q_{ii}=0
$$
Odwzorowanie podobieństwa przebiega poprzez cykliczne aktualizacje punktów $y_j$ w przestrzeni niskowymiarowej poprzez minimalizację dywergencji KL:

$$
\mathrm{KL}(P \| Q)=\sum_{i \neq j} p_{i j} \log \frac{p_{i j}}{q_{i j}}
$$
```{r}
mnist <- read_csv("mnist.csv")
# Wizualizacja przykładowych zdjęć
xy_axis <- data.frame(x = expand.grid(1:28,28:1)[,1],
                      y = expand.grid(1:28,28:1)[,2])
plot_theme <- list(
  raster = geom_raster(hjust = 0, vjust = 0),
  gradient_fill = scale_fill_gradient(low = "white", high = "black", guide = FALSE),
  theme = theme(axis.line = element_blank(),
                axis.text = element_blank(),
                axis.ticks = element_blank(),
                axis.title = element_blank(),
                panel.background = element_blank(),
                panel.border = element_blank(),
                panel.grid.major = element_blank(),
                panel.grid.minor = element_blank(),
                plot.background = element_blank())
)

sample_plots <- sample(1:nrow(mnist), 100) %>% map(~ {
  plot_data <- cbind(xy_axis, fill = t(mnist[.x, -1]))
  ggplot(plot_data, aes(x, y, fill = fill)) + plot_theme
})

do.call("grid.arrange", c(sample_plots, ncol = 10, nrow = 10))

# PCA
pca <- prcomp(mnist[ , -1])
pca_mnist <- pca$x[ , 1:2] %>% as.data.frame() %>%
  bind_cols(., label = factor(mnist$label))
ggplot(pca_mnist, aes(PC1, PC2, color = label)) + theme_bw() + geom_point()

# t-SNE
tsne <- Rtsne(mnist[ , -1],
              dims = 2, # Na ile wymiarów chcemy rzutować nasze dane
              perplexity = 30,
              verbose = TRUE,
              pca = TRUE, # Czy wykonać PCA na początku ?
              max_iter = 500) # Liczba iteracji algorytmu
tsne_mnist <- tsne$Y %>% as.data.frame() %>%
  dplyr::rename(x = V1, y = V2) %>% bind_cols(., label = factor(mnist$label))
ggplot(tsne_mnist, aes(x, y, color = label)) + theme_bw() + geom_point()
```

Zalety t-SNE:
 - działa dla mocno nieliniowych, nieustrukturyzowanych danych
 - zachowuje lokalne i globalne struktury

Wady t-SNE
 - jako, że bazuje na odległościach pomiędzy punktami (wszystkimi) nie dostejemy możliwości przeliczenia dla nowej obserwacji (brak wzoru)
 - konieczność sprawdzenia wielu wartości `perplexity` i `max_iter`
 